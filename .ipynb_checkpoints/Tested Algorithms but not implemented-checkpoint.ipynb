{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437fd2e9",
   "metadata": {},
   "source": [
    "### In this notebook I am keeping the codes/ algorithms which I tried to make observations such as \n",
    "- Smith-WaterMan Algorithm\n",
    "- Sørensen-Dice Coefficient\n",
    "- n-gram algorithm\n",
    "- even other method of applying LCS algorithms\n",
    "\n",
    "\n",
    "these algorithms are kept here so that i can have access at latter point of time.\n",
    "\n",
    "#### Also i will keep on updating this notebook for algorithms which i tested but didn't used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fbfd63",
   "metadata": {},
   "source": [
    "### 1. LCS using SequenceMatcher form difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # Tokenize, remove punctuation, and convert to lowercase\n",
    "    tokens = sentence.lower().split()\n",
    "    return [token.strip(\".,!?\") for token in tokens]\n",
    "\n",
    "def lcs_similarity(sentence1, sentence2):\n",
    "    tokens1 = preprocess_sentence(sentence1)\n",
    "    tokens2 = preprocess_sentence(sentence2)\n",
    "\n",
    "    # Calculate the LCS using the SequenceMatcher\n",
    "    seq_matcher = SequenceMatcher(None, tokens1, tokens2)\n",
    "    lcs = seq_matcher.find_longest_match(0, len(tokens1), 0, len(tokens2))\n",
    "\n",
    "    # Calculate the LCS length and total length of sentence2\n",
    "    lcs_length = lcs.size\n",
    "    total_length = len(tokens2)\n",
    "\n",
    "    return lcs_length, total_length\n",
    "\n",
    "def find_unique_content(sentence1, sentence2):\n",
    "    lcs_length, total_length = lcs_similarity(sentence1, sentence2)\n",
    "    threshold = 0.6  # Adjust as needed\n",
    "\n",
    "    if lcs_length <= threshold * total_length:\n",
    "        # Filter out the unique content from sentence2\n",
    "        tokens1 = preprocess_sentence(sentence1)\n",
    "        tokens2 = preprocess_sentence(sentence2)\n",
    "        unique_tokens = [token for token in tokens2 if token not in tokens1]\n",
    "        return \" \".join(unique_tokens)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example sentences\n",
    "main_sentence = \"\"\"You Dat Unc Atte Unc Mir Lar Nat Unt ash ccr < > C 0 amazon in/s?k=rain+water&crid= ZPROALJK3C2O9&sprefix=rain-water\"2Caps%62C2588ref=nb_sb_noss Relaunch to updlate Favorites Fowdo steppern_ Raspperry Pi Tutori_ DealeXtreme Cccl; https;/www,course Water Leve Control Projects Working principle Simple Prcximity Se_ Welcomel Welcom  Career Ropotics AIl Bookmarks amazon Deliver to Annu rain water Hello, Maninder Retums Subtotal Sirsa 125055 EN Account & Lists & Orders Cart {549.00 prime Great Indian Festival Cart =Al Car & Motorbike Amazon miniTV Sell Baby Health; Household & Personal Care Gift Cards Pet Supplies Subscribe & Save Browsing History Customer Service SHOP NOW Finale days \"3,117 MRP:34599 (32% off) Save extra with No Cost EMI FREE Delivery polDf BOLDRT {315 MRP:4399 (19% off) prime FREE Delivery by Sunday, 12 FREE Delivery by Monday; 13 November 'prime FREE Delivery by Wednesday, 8 November for Prime members November {549.00 prime Rainy ZEXEL Snuggling Cloud Rain NeeRain SS 304 Rooftop Rainwater Rainy FL - 500 Self-Cleaning Dual RainGain Rainwater Harvesting Filter SPIRY FI ENGINEERS Aqua 5 Micron Diffuser; Snuggle Cloud; Raindrop Harvesting Filter; Pack of Intensity Rainwater Harvesting Filter (Stainless Steel Filter Mesh; 75 mm) Dia & 20\" Length Bag Bore Water Humidifier; Mushroom Waterfall Suitable for Area Upto 5400 Square 47y7 Filter Lamp; Anxiety and Stress Relief_ Feet; Black Installation Kit Great Indian Festival 463 *6,500 300+ bought in month Save extra with No Cost EMI 3,250MRP:43958 (18% off) 2,099MRP: #2999 (30%/ off) 'prime FREE Delivery by Saturday, 11 *26,500 Save extra with No Cost EMI \"280 MRP: #599 (53%/ off) Save <35 with coupon November Save extra with No Cost EMI Save 5% with coupon FREE Delivery by Monday, 13 November prime FREE Delivery by Saturday; 11 FREE Delivery by Monday; 13 November for prime FREE Delivery by Sunday, 12 Prime members November for Prime members November Rain Water Collection By Blake Holliday 10.40 06-11-2023 New? Hicrol Past\"\"\"\n",
    "other_sentence = \"\"\"You Dat Unc Atte Unc Mir Lar Nat Unt ash ccr < > C 0 amazon in/s?k=rain+water&crid= ZPROALJK3C2O9&sprefix=rain+water\"2Caps%2C258&ref-nb_sb_noss Relaunch to updlate Favorites Fowdo steppern_ Raspperry Pi Tutori_ DealeXtreme Cccl; https;/www,course Water Leve Control Projects Working principle Simple Prcximity Se_ Welcomel Welcom  Career Ropotics AIl Bookmarks amazon Deliver to Annu rain water Hello, Maninder Retums Subtotal Sirsa 125055 EN Account & Lists & Orders Cart 7549.00 prime Cart =Al Car & Motorbike Amazon miniTV Sell Baby Health; Household & Personal Care Gift Cards Pet Supplies Subscribe & Save Browsing History Customer Service at Indian Festival SHOP NOW Finale days polDf BOLDRT ZEXEL Snuggling Cloud Rain NeeRain SS 304 Rooftop Rainwater Rainy FL 500 Self-Cleaning Dual RainGain Rainwater Harvesting Filter SPIRY FI ENGINEERS Aqua 5 Micron {549.00 Diffuser; Snuggle Cloud; Raindrop Harvesting Filter; Pack of Intensity Rainwater Harvesting Filter (Stainless Steel Filter Mesh; 75 mm) Dia & 20\" Length Bag Bore Water prime Humidifier; Mushroom Waterfall Suitable for Area 5400 Square Filter Lamp, Anxiety and Stress Relief; Feet; Black Installation Kit Great Indian Festival 463 '6,500 300+ bought in month Save extra with No Cost EMI {3,250MRP:43958 (18% off) '2,099 MRP: 42,999 (30%/ off) prime FREE Delivery by Saturday; 11 '26,500 Save extra with No Cost EMI '280MRP: #599 (53% off) Save <35 with coupon November Save extra with No Cost EMI Save 5% with coupon FREE Delivery by Monday, 13 November prime FREE Delivery by Saturday, 11 FREE Delivery by Monday; 13 November for (prime FREE Delivery by Sunday, 12 Prime members November for Prime members November Rain Water Collection By Blake Holliday K FITG18 Mens Raincoat with Hood Rain Cloud Humidifier Water Drip, 2 Rain Water Collection (Foxhole KOHLER Rain Shower Round 203mm UJEAVETTE@ Rain Cloud Humidifier Water Fighter-Rain Coat for Men Humidifier with Essential Oil Homestead Book 11) (Rain Duet); with Katalyst Air- Water Drip Colors Night Light Waterproof Pant and Carrying Diffuser;45Oml Cloud Humidifier by Blake Holliday induction Spray Technology Mushroom Lamp Diffuser Grain Pouch; Navy Blue Rain Drop, Mushroom Humidifier_ (Polished Chrome) Green Kindle Edition *8,099 MRP: 213498 (39% off) {415MRP: =699 (41% otf) 11,124 *0 kindle unlimited Great Indian Festival Save extra with No Cost EMI Free with Kindle Unlimited membership prime Save extra with No Cost EMI \"6,450MRP: #8-688 (25% off) Get it Tuesday, 21 November Friday, 24 Learn More https:/ /www amazon in/events greatindianfestivaV&ref_=ine_gw_j23_03_swm/?_encoding-UTF8Rref_=nav_swm_SK= Fridav 24 Movember - Tuesdav 28 ith Ao Cost FM November 10.40 06-11-2023 New? Upto Past Geip\"\"\"\n",
    "\n",
    "unique_content = find_unique_content(main_sentence, other_sentence)\n",
    "if unique_content:\n",
    "    print(\"Unique content:\", unique_content)\n",
    "else:\n",
    "    print(\"No unique content found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4acd0d",
   "metadata": {},
   "source": [
    "## 2.  Smith-Waterman Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0986cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # Tokenize, remove punctuation, and convert to lowercase\n",
    "    tokens = sentence.lower().split()\n",
    "    return [token.strip(\".,!?\") for token in tokens]\n",
    "\n",
    "def smith_waterman_similarity(sentence1, sentence2):\n",
    "    tokens1 = preprocess_sentence(sentence1)\n",
    "    tokens2 = preprocess_sentence(sentence2)\n",
    "\n",
    "    # Convert token lists back to strings\n",
    "    tokens1_str = \" \".join(tokens1)\n",
    "    tokens2_str = \" \".join(tokens2)\n",
    "\n",
    "    # Calculate the Smith-Waterman alignment score\n",
    "    alignments = pairwise2.align.localxx(tokens1_str, tokens2_str, one_alignment_only=True)\n",
    "    best_alignment = alignments[0]\n",
    "    best_alignment_score = best_alignment[2]\n",
    "\n",
    "    return best_alignment_score\n",
    "\n",
    "def find_unique_content(sentence1, sentence2):\n",
    "    sw_score = smith_waterman_similarity(sentence1, sentence2)\n",
    "    threshold = 0.6  # Adjust as needed\n",
    "\n",
    "    if sw_score <= threshold:\n",
    "        # Extract the best-matching substring from sentence2\n",
    "        tokens1 = preprocess_sentence(sentence1)\n",
    "        tokens2 = preprocess_sentence(sentence2)\n",
    "        tokens1_str = \" \".join(tokens1)\n",
    "        tokens2_str = \" \".join(tokens2)\n",
    "        best_alignment = pairwise2.align.localxx(tokens1_str, tokens2_str, one_alignment_only=True)[0]\n",
    "        best_matching_substring = tokens2[best_alignment[3]:best_alignment[4]]\n",
    "\n",
    "        # Convert the substring tokens back to a string\n",
    "        best_matching_substring_str = \" \".join(best_matching_substring)\n",
    "\n",
    "        # Filter out the unique content from sentence2\n",
    "        unique_content = sentence2.replace(best_matching_substring_str, \"\")\n",
    "        return unique_content.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example sentences\n",
    "main_sentence = \"This is the main sentence.\"\n",
    "other_sentence = \"This is a sentence with some unique content.\"\n",
    "\n",
    "unique_content = find_unique_content(main_sentence, other_sentence)\n",
    "if unique_content:\n",
    "    print(\"Unique content:\", unique_content)\n",
    "else:\n",
    "    print(\"No unique content found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075bf0e",
   "metadata": {},
   "source": [
    "## 3. Sorensen-Dice coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c506506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Tokenize, remove punctuation, and convert to lowercase\n",
    "    tokens = sentence.lower().split()\n",
    "    return set([token.strip(\".,!?\") for token in tokens])\n",
    "\n",
    "def dice_coefficient_similarity(sentence1, sentence2):\n",
    "    # Preprocess the sentences\n",
    "    tokens1 = preprocess_sentence(sentence1)\n",
    "    tokens2 = preprocess_sentence(sentence2)\n",
    "\n",
    "    # Calculate the Sørensen-Dice Coefficient\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    dice_coefficient = (2.0 * intersection) / (len(tokens1) + len(tokens2))\n",
    "\n",
    "    return dice_coefficient\n",
    "\n",
    "def find_unique_content(sentence1, sentence2):\n",
    "    dice_similarity = dice_coefficient_similarity(sentence1, sentence2)\n",
    "    threshold = 0.6  # Adjust as needed\n",
    "\n",
    "    if dice_similarity <= threshold:\n",
    "        # Extract the unique content\n",
    "        unique_tokens = preprocess_sentence(sentence2) - preprocess_sentence(sentence1)\n",
    "        unique_content = \" \".join(unique_tokens)\n",
    "\n",
    "        return unique_content\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example sentences\n",
    "main_sentence = \"This is the main sentence.\"\n",
    "other_sentence = \"This is a sentence with some unique content.\"\n",
    "\n",
    "unique_content = find_unique_content(main_sentence, other_sentence)\n",
    "if unique_content:\n",
    "    print(\"Unique content:\", unique_content)\n",
    "else:\n",
    "    print(\"No unique content found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90bd22",
   "metadata": {},
   "source": [
    "## 4. n-gram() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # Tokenize and preprocess the sentence\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    return tokens\n",
    "\n",
    "def calculate_ngram_similarity(sentence1, sentence2, n):\n",
    "    # Preprocess the sentences\n",
    "    tokens1 = preprocess_sentence(sentence1)\n",
    "    tokens2 = preprocess_sentence(sentence2)\n",
    "\n",
    "    # Generate n-grams for each sentence\n",
    "    ngrams1 = list(ngrams(tokens1, n))\n",
    "    ngrams2 = list(ngrams(tokens2, n))\n",
    "\n",
    "    # Calculate Jaccard similarity between n-grams\n",
    "    set1 = set(ngrams1)\n",
    "    set2 = set(ngrams2)\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    jaccard_similarity = intersection / union\n",
    "\n",
    "    return jaccard_similarity\n",
    "\n",
    "def find_unique_content(sentence1, sentence2, n, threshold):\n",
    "    ngram_similarity = calculate_ngram_similarity(sentence1, sentence2, n)\n",
    "\n",
    "    if ngram_similarity <= threshold:\n",
    "        # Extract the unique content\n",
    "        unique_tokens = set(preprocess_sentence(sentence2)) - set(preprocess_sentence(sentence1))\n",
    "        unique_content = \" \".join(unique_tokens)\n",
    "\n",
    "        return unique_content\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example sentences\n",
    "main_sentence = \"This is the main sentence.\"\n",
    "other_sentence = \"This is a sentence with some unique content.\"\n",
    "\n",
    "n = 3  # Adjust the n-gram size as needed\n",
    "threshold = 0.6  # Adjust as needed\n",
    "\n",
    "unique_content = find_unique_content(main_sentence, other_sentence, n, threshold)\n",
    "if unique_content:\n",
    "    print(\"Unique content:\", unique_content)\n",
    "else:\n",
    "    print(\"No unique content found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
